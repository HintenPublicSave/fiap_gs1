# -*- coding: utf-8 -*-
"""treinamento_modelo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WwRQTPN16PiR8sA0YuryWt4t3OjU6nAi

ğŸ‘¨â€ğŸ“ Integrantes:

Alice C. M. Assis - RM 566233

Leonardo S. Souza - RM 563928

Lucas B. Francelino - RM 561409

Pedro L. T. Silva - RM 561644

Vitor A. Bezerra - RM 563001

GitHub:

https://github.com/Hinten/fiap_gs1

ğŸ” ImportaÃ§Ã£o de bibliotecas especializadas para anÃ¡lise de dados agrÃ­colas

Nesta cÃ©lula, sÃ£o importadas bibliotecas essenciais para anÃ¡lise de dados e prÃ©-processamento e modelagem (sklearn). Essas ferramentas fornecem funcionalidades fundamentais para manipulaÃ§Ã£o de datasets, transformaÃ§Ã£o de variÃ¡veis e construÃ§Ã£o de modelos preditivos eficientes.
"""

# ğŸ” ImportaÃ§Ã£o de bibliotecas especializadas para anÃ¡lise de dados agrÃ­colas

# ------------------------------
# ğŸ“Š ManipulaÃ§Ã£o e VisualizaÃ§Ã£o
# ------------------------------
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# ------------------------------
# ğŸ”„ PrÃ©-processamento
# ------------------------------
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, StratifiedKFold

# ------------------------------
# ğŸ¤– Modelagem (Classificadores)
# ------------------------------
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import (
    RandomForestClassifier,
    GradientBoostingClassifier,
    AdaBoostClassifier,
    BaggingClassifier,
    ExtraTreesClassifier
)
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.calibration import CalibratedClassifierCV

# ------------------------------
# ğŸ§ª AvaliaÃ§Ã£o de Desempenho
# ------------------------------
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score
)

# ------------------------------
# âš™ï¸ UtilitÃ¡rios
# ------------------------------
import random
import time
import os

"""ğŸ“‚ DefiniÃ§Ã£o do caminho de acesso aos dados da lavoura

Aqui, o caminho do arquivo .csv contendo os dados agrÃ­colas (com as colunas Data, Cota, Chuva e Nivel) Ã© definido em uma variÃ¡vel. Essa prÃ¡tica torna o cÃ³digo mais organizado e permite reutilizar facilmente o caminho do arquivo ao longo do notebook, facilitando ajustes e reaproveitamento do script.
"""

#ğŸ“‚ DefiniÃ§Ã£o do caminho de acesso aos dados da lavoura

# fonte dos dados: https://github.com/anagovbr/hidro-dados-estacoes-convencionais/tree/main/fluviometricas
csv_path = "COTAxCHUVA.csv"

"""ğŸ“¥ Carregamento do dataset para estrutura tabular do pandas

Esta etapa carrega os dados do arquivo CSV para um DataFrame, que Ã© a estrutura de dados mais comum do pandas. Isso permite a exploraÃ§Ã£o, limpeza e anÃ¡lise dos dados referentes Ã s mediÃ§Ãµes de nÃ­veis de Ã¡gua (Cota), chuva e data, alÃ©m da classificaÃ§Ã£o em nÃ­veis de atenÃ§Ã£o (Nivel).
"""

#ğŸ“¥ Carregamento do dataset para estrutura tabular do pandas

df = pd.read_csv(csv_path)

"""ğŸ“ˆ Estimativa dos nÃ­veis crÃ­ticos de cota para monitoramento

Nesta cÃ©lula, sÃ£o calculados os valores dos percentis 90 (P90), 95 (P95) e 98 (P98) da coluna 'Cota'. Esses percentis servem para estabelecer limites de referÃªncia para os nÃ­veis de atenÃ§Ã£o, alerta e inundaÃ§Ã£o provÃ¡vel na anÃ¡lise das cotas de Ã¡gua. Os valores calculados ajudam a definir pontos crÃ­ticos para aÃ§Ãµes preventivas na gestÃ£o da lavoura e monitoramento ambiental.
"""

#ğŸ“ˆ Estimativa dos nÃ­veis crÃ­ticos de cota para monitoramento

# Estimar cotas de atenÃ§Ã£o, alerta e inundaÃ§Ã£o provÃ¡vel
cota_atencao = df['Cota'].quantile(0.90)
cota_alerta = df['Cota'].quantile(0.95)
cota_inundacao = df['Cota'].quantile(0.98)

print(f"Cota de atenÃ§Ã£o (P90): {cota_atencao:.2f}")
print(f"Cota de alerta (P95): {cota_alerta:.2f}")
print(f"Cota de inundaÃ§Ã£o provÃ¡vel (P98): {cota_inundacao:.2f}")

"""âš ï¸ DefiniÃ§Ã£o dos limites de risco e classificaÃ§Ã£o do nÃ­vel de inundaÃ§Ã£o

Nesta etapa, sÃ£o definidos os valores fixos para os limites dos nÃ­veis de atenÃ§Ã£o, alerta e inundaÃ§Ã£o provÃ¡vel baseados na variÃ¡vel 'Cota'.

A funÃ§Ã£o `classificar_nivel` recebe o valor da cota e classifica cada registro do DataFrame em uma das categorias de risco:
-*InundaÃ§Ã£o provÃ¡vel*: cota acima do limite de inundaÃ§Ã£o;
-*Alerta elevado*: cota acima do limite de alerta, mas abaixo da inundaÃ§Ã£o;
-*SituaÃ§Ã£o de atenÃ§Ã£o*: cota acima do limite de atenÃ§Ã£o, mas abaixo do alerta;
-*CondiÃ§Ãµes normais*: valores abaixo do limite de atenÃ§Ã£o.

Por fim, essa classificaÃ§Ã£o Ã© aplicada para criar uma nova coluna chamada 'Nivel' no DataFrame, que serÃ¡ usada como variÃ¡vel alvo para anÃ¡lises e modelos preditivos.
"""

#âš ï¸ DefiniÃ§Ã£o dos limites de risco e classificaÃ§Ã£o do nÃ­vel de inundaÃ§Ã£o

# Definindo os limites
limite_atencao = 205
limite_alerta = 250
limite_inundacao = 315

# Criar nova coluna 'Nivel'
def classificar_nivel(cota):
    if cota > limite_inundacao:
        return 'InundaÃ§Ã£o provÃ¡vel'
    elif cota > limite_alerta:
        return 'Alerta elevado'
    elif cota > limite_atencao:
        return 'SituaÃ§Ã£o de atenÃ§Ã£o'
    else:
        return 'CondiÃ§Ãµes normais'

df['Nivel'] = df['Cota'].apply(classificar_nivel)

"""ğŸ” InspeÃ§Ã£o inicial do dataset para compreensÃ£o de variÃ¡veis e formato

Visualizar as primeiras linhas do DataFrame com df.head() serve como um ponto de partida para compreender a estrutura do dataset, verificar nomes de colunas, tipos de variÃ¡veis (como Data, Cota, Chuva e Inundacao) e identificar possÃ­veis anomalias logo no inÃ­cio.
"""

#ğŸ” InspeÃ§Ã£o inicial do dataset para compreensÃ£o de variÃ¡veis e formato

# 4.1) Exibir primeiras linhas do dataset
df.head(1000)

"""ğŸ§¾ DiagnÃ³stico estrutural do DataFrame e tipagem dos dados com df.info()

Utiliza-se o mÃ©todo df.info() para obter um resumo tÃ©cnico da estrutura do dataset. Essa funÃ§Ã£o retorna:

- NÃºmero total de entradas (linhas);
- NÃºmero e o nome das colunas;
- NÃºmero de valores nÃ£o nulos em cada coluna;
- Tipo de dado de cada coluna (int64, float64, object, etc.);
- Uso aproximado de memÃ³ria.

Essa anÃ¡lise Ã© essencial para validar se:

Todas as colunas foram corretamente interpretadas pelo pandas (ex: float ao invÃ©s de object), se existem colunas com valores ausentes (non-null < total) e se hÃ¡ necessidade de otimizar tipos de dados para uso eficiente de memÃ³ria, principalmente em grandes volumes de dados. Essa inspeÃ§Ã£o ajuda a antecipar problemas e tomar decisÃµes sobre prÃ©-processamento antes da anÃ¡lise ou modelagem.
"""

#ğŸ§¾ DiagnÃ³stico estrutural do DataFrame e tipagem dos dados com df.info() e Complementos Ãºteis (comentados):

df.info()

"""ğŸš¨ DetecÃ§Ã£o de dados faltantes para avaliaÃ§Ã£o da necessidade de imputaÃ§Ã£o

Esta etapa verifica a presenÃ§a de valores nulos no dataset utilizando df.isnull().sum(). Embora a funÃ§Ã£o df.info() tambÃ©m identifique dados faltantes, sua visualizaÃ§Ã£o pode ser menos intuitiva. A existÃªncia de valores ausentes pode prejudicar tanto a anÃ¡lise exploratÃ³ria quanto o desempenho dos modelos preditivos. Por isso, Ã© fundamental decidir uma estratÃ©gia adequada, como imputaÃ§Ã£o, exclusÃ£o de linhas ou colunas, ou o uso de modelos que suportem dados nulos.
"""

#ğŸš¨ DetecÃ§Ã£o de dados faltantes para avaliar necessidade de imputaÃ§Ã£o

print(df.isnull().sum())

"""ğŸ”„ Preenchimento de valores faltantes por interpolaÃ§Ã£o pelo valor mais prÃ³ximo

Nesta etapa, utilizamos o mÃ©todo de interpolaÃ§Ã£o 'nearest' para preencher valores ausentes (NaN) no DataFrame. Esse mÃ©todo substitui os valores faltantes pelo valor vÃ¡lido mais prÃ³ximo, seja ele anterior ou posterior, preservando a coerÃªncia dos dados temporais ou sequenciais. ApÃ³s o preenchimento, Ã© realizada uma verificaÃ§Ã£o para confirmar se ainda existem valores nulos no dataset, garantindo que a interpolaÃ§Ã£o foi aplicada com sucesso.
"""

#ğŸ”„ Preenchimento de valores faltantes por interpolaÃ§Ã£o pelo valor mais prÃ³ximo

df = df.interpolate(method='nearest')

# Confirmando se ainda ha dados faltantes
print(df.isnull().sum())

"""ğŸ” DetecÃ§Ã£o de entradas redundantes no dataset

Esta cÃ©lula tem como objetivo identificar registros duplicados, ou seja, linhas que aparecem mais de uma vez com os mesmos valores em todas as colunas. Isso Ã© feito por meio do mÃ©todo df.duplicated(), que retorna uma sÃ©rie booleana indicando True para as linhas duplicadas.
"""

#ğŸ” DetecÃ§Ã£o de entradas redundantes no dataset
duplicates= df.duplicated().sum()
print(f"Total de duplicados: {duplicates}")

"""ğŸ“Š DetecÃ§Ã£o e tratamento de valores extremos

Esta cÃ©lula tem o objetivo de identificar e tratar outliers (valores fora do padrÃ£o esperado) nas colunas 'Cota' e 'Chuva', tratar outliers Ã© essencial para evitar distorÃ§Ãµes em modelos de regressÃ£o, classificadores e estatÃ­sticas descritivas, especialmente em modelos sensÃ­veis a valores extremos, como KNN e regressÃ£o linear.
"""

# ğŸ“Œ VerificaÃ§Ã£o de valores extremos fora do padrÃ£o esperado

# Limites lÃ³gicos (ajuste conforme o contexto da sua bacia)
limites_explicitos = {
    'Cota': (50, 500),
    'Chuva': (0, 300)
}

for coluna in limites_explicitos:
    limite_inferior, limite_superior = limites_explicitos[coluna]

    extremos = df[(df[coluna] < limite_inferior) | (df[coluna] > limite_superior)]

    print(f"\nğŸ” Verificando extremos na coluna: {coluna}")
    print(f"Limites esperados: {limite_inferior} a {limite_superior}")
    print(f"NÃºmero de valores fora do padrÃ£o: {len(extremos)}")

    if not extremos.empty:
        print(extremos[[coluna]].head())

"""ğŸ“Š EstatÃ­sticas descritivas para anÃ¡lise quantitativa preliminar

Com df.describe(), obtemos medidas estatÃ­sticas como mÃ©dia, mediana, desvio padrÃ£o e quartis para variÃ¡veis numÃ©ricas. Isso fornece uma noÃ§Ã£o do comportamento e da dispersÃ£o dos dados, ajudando a identificar outliers e padrÃµes.
"""

#ğŸ“Š EstatÃ­sticas descritivas para anÃ¡lise quantitativa preliminar

df.describe()

"""ğŸ¯ QuantificaÃ§Ã£o das classes da variÃ¡vel alvo

A funÃ§Ã£o df['Nivel'].value_counts() permite entender quantas amostras existem para cada entrada no dataset.
"""

#ğŸ¯ QuantificaÃ§Ã£o das classes da variÃ¡vel alvo

df['Nivel'].value_counts()

"""ğŸ¯ Separando os dados em variÃ¡veis preditoras e variÃ¡vel target

Nesta etapa, fazemos a divisÃ£o dos dados em:

X: variÃ¡veis preditoras (features), que contÃªm as informaÃ§Ãµes sobre Chuvas e Cotas. Essas serÃ£o as entradas para os modelos.
y: variÃ¡vel target (rÃ³tulo), que indica o nÃ­vel de enchente que queremos prever.

Essa separaÃ§Ã£o Ã© fundamental para o treinamento dos modelos de machine learning, pois permite que eles aprendam a relaÃ§Ã£o entre as condiÃ§Ãµes do ambiente (X) e o nÃ­vel de enchente (y). Assim, podemos avaliar a capacidade preditiva dos algoritmos para identificar o risco de inundaÃ§Ã£o com base nas condiÃ§Ãµes observadas.
"""

#ğŸ¯ Separando os dados em variÃ¡veis preditoras e variÃ¡vel target

X = df.drop(columns=['Nivel'])
y = df['Nivel']

"""ğŸ¯ ConversÃ£o da variÃ¡vel alvo categÃ³rica para formato numÃ©rico

Para que os algoritmos de machine learning possam processar a variÃ¡vel alvo 'Nivel', convertemos as categorias textuais para valores numÃ©ricos usando o LabelEncoder. Isso facilita o treinamento dos modelos.
"""

#ğŸ¯ ConversÃ£o da variÃ¡vel alvo categÃ³rica para formato numÃ©rico

le = LabelEncoder()
y_enc = le.fit_transform(y)

df['Nivel'] = y_enc

"""âœ‚ï¸ SeparaÃ§Ã£o dos dados em conjuntos de treino e teste com estratificaÃ§Ã£o

Aqui os dados sÃ£o divididos em conjuntos de treinamento e teste com base em uma proporÃ§Ã£o definida (80/20). O parÃ¢metro stratify=y garante que a distribuiÃ§Ã£o das classes da variÃ¡vel alvo seja preservada em ambas as amostras, o que Ã© fundamental para garantir avaliaÃ§Ãµes mais realistas e imparciais dos modelos.
"""

#âœ‚ï¸ SeparaÃ§Ã£o dos dados em conjuntos de treino e teste com estratificaÃ§Ã£o

y = df['Nivel']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""ğŸ“ Escalonamento das variÃ¡veis numÃ©ricas para melhorar desempenho dos modelos

A normalizaÃ§Ã£o dos dados, feita com MinMaxScaler, ajusta as variÃ¡veis para uma mesma escala (geralmente de 0 a 1). Isso Ã© crucial para algoritmos que sÃ£o sensÃ­veis Ã  magnitude dos dados, como KNN e SVM, garantindo que nenhuma variÃ¡vel domine a modelagem apenas por ter valores maiores.
"""

#ğŸ“ Escalonamento das variÃ¡veis numÃ©ricas para melhorar desempenho dos modelos

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Garante que y Ã© 1D
y_train = np.ravel(y_train)
y_test = np.ravel(y_test)

# Garante que os valores sÃ£o inteiros
y_train = y_train.astype(int)
y_test = y_test.astype(int)

"""ğŸ”„ Configurar validaÃ§Ã£o cruzada estratificada com StratifiedKFold

Aqui, utilizamos o StratifiedKFold para criar 5 divisÃµes (folds) dos dados que preservam a proporÃ§Ã£o original das classes em cada parte. Configuramos o embaralhamento dos dados (shuffle=True) para garantir aleatoriedade na divisÃ£o e definimos uma semente fixa (random_state=42) para resultados reproduzÃ­veis, essa configuraÃ§Ã£o assegura que o modelo seja avaliado de forma equilibrada e consistente em diferentes subconjuntos do conjunto de dados.
"""

#ğŸ”„ Configurar validaÃ§Ã£o cruzada estratificada com StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

"""âš™ï¸ InstanciaÃ§Ã£o de modelos de aprendizado supervisionado para classificaÃ§Ã£o

Nesta cÃ©lula, criamos uma lista de 20 modelos de machine learning para classificaÃ§Ã£o, com diferentes algoritmos e hiperparÃ¢metros aleatÃ³rios. Os modelos incluem RegressÃ£o LogÃ­stica, Ãrvore de DecisÃ£o, Random Forest, Gradient Boosting, SVM, KNN e Naive Bayes.

Para garantir diversidade, cada modelo recebe um nome Ãºnico baseado em seus hiperparÃ¢metros, evitando duplicatas. Essa variedade permite testar diferentes configuraÃ§Ãµes e comparar seu desempenho na tarefa de prever se haverÃ¡ ou nÃ£o inundaÃ§Ãµes.
"""

#âš™ï¸ InstanciaÃ§Ã£o de modelos de aprendizado supervisionado para classificaÃ§Ã£o

# FunÃ§Ãµes auxiliares para gerar variaÃ§Ãµes aleatÃ³rias
random_state = lambda: random.randint(1, 100)
n_estimators = lambda: random.choice([50, 100, 150, 200])
k_neighbors = lambda: random.choice([3, 5, 7, 10, 15])
max_depth = lambda: random.choice([None, 3, 5, 10])
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
hidden_layer_sizes = lambda: random.choice([(50,), (100,), (50, 50), (100, 50)])

modelos = []
nomes_gerados = set()

while len(modelos) < 20:
    modelo_tipo = random.choice([
        'lr', 'dt', 'rf', 'gb', 'svm', 'knn', 'nb',
        'et', 'ada', 'lda', 'qda', 'mlp', 'bag', 'cal'
    ])

    if modelo_tipo == 'lr':
        nome = f'LogReg {random_state()}'
        if nome not in nomes_gerados:
            modelo = LogisticRegression(max_iter=1000, random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'dt':
        depth = max_depth()
        nome = f'DecTree d{depth}'
        if nome not in nomes_gerados:
            modelo = DecisionTreeClassifier(max_depth=depth, random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'rf':
        n = n_estimators()
        nome = f'RandForest {n}'
        if nome not in nomes_gerados:
            modelo = RandomForestClassifier(n_estimators=n, random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'gb':
        n = n_estimators()
        nome = f'GradBoost {n}'
        if nome not in nomes_gerados:
            modelo = GradientBoostingClassifier(n_estimators=n, random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'et':
        n = n_estimators()
        nome = f'ExtraTrees {n}'
        if nome not in nomes_gerados:
            modelo = ExtraTreesClassifier(n_estimators=n, random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'ada':
        n = n_estimators()
        nome = f'AdaBoost {n}'
        if nome not in nomes_gerados:
            modelo = AdaBoostClassifier(n_estimators=n, random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'svm':
        kernel = random.choice(kernels)
        nome = f'SVM {kernel}'
        if nome not in nomes_gerados:
            modelo = SVC(kernel=kernel, probability=True, random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'knn':
        k = k_neighbors()
        nome = f'KNN {k}'
        if nome not in nomes_gerados:
            modelo = KNeighborsClassifier(n_neighbors=k)
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'nb':
        nome = f'Naive Bayes {random_state()}'
        if nome not in nomes_gerados:
            modelo = GaussianNB()
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'lda':
        nome = 'LDA'
        if nome not in nomes_gerados:
            modelo = LinearDiscriminantAnalysis()
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'qda':
        nome = 'QDA'
        if nome not in nomes_gerados:
            modelo = QuadraticDiscriminantAnalysis()
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'mlp':
        hls = hidden_layer_sizes()
        nome = f'MLP {hls}'
        if nome not in nomes_gerados:
            modelo = MLPClassifier(hidden_layer_sizes=hls, max_iter=1000, random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'bag':
        base_depth = max_depth()
        nome = f'Bagging DT d{base_depth}'
        if nome not in nomes_gerados:
            base_est = DecisionTreeClassifier(max_depth=base_depth, random_state=random_state())
            modelo = BaggingClassifier(estimator=base_est, n_estimators=n_estimators(), random_state=random_state())
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

    elif modelo_tipo == 'cal':
        # CalibratedClassifierCV precisa de base, usar SVM linear
        nome = 'Calibrated SVM linear'
        if nome not in nomes_gerados:
            base_svm = SVC(kernel='linear', probability=False, random_state=random_state())
            modelo = CalibratedClassifierCV(base_svm)
            modelos.append((nome, modelo))
            nomes_gerados.add(nome)

"""ğŸ“Š Treinamento dos modelos, validaÃ§Ã£o cruzada e avaliaÃ§Ã£o preditiva

Nesta cÃ©lula, executa-se o ciclo completo de aprendizado de mÃ¡quina para cada modelo instanciado:

- â±ï¸ *Treinamento* com os dados de treino (`X_train`, `y_train`);
- ğŸ” *PrediÃ§Ã£o* e avaliaÃ§Ã£o no conjunto de teste (`X_test`, `y_test`);
- ğŸ“ˆ *Coleta de mÃ©tricas preditivas* como AcurÃ¡cia, PrecisÃ£o, Recall, F1-Score e ROC AUC;
- â³ *Registro do tempo de treinamento* de cada modelo;
- ğŸ“¦ Armazenamento dos modelos treinados e de suas prediÃ§Ãµes para uso posterior.

Essa abordagem permite comparar o desempenho e a eficiÃªncia de mÃºltiplos algoritmos sob as mesmas condiÃ§Ãµes experimentais.
"""

#ğŸ“Š Treinamento dos modelos, validaÃ§Ã£o cruzada e avaliaÃ§Ã£o preditiva


# AvaliaÃ§Ã£o dos modelos
resultados = []
tempos = []
parametros = []
modelos_treinados = {}
y_preds = {}
resultados = []

for nome, modelo in modelos:
    print(f"Treinando: {nome}")
    inicio = time.time()

    modelo.fit(X_train, y_train)
    y_pred = modelo.predict(X_test)

    fim = time.time()
    duracao = fim - inicio

    try:
        y_proba = modelo.predict_proba(X_test)
        if len(set(y)) == 2:
            auc = roc_auc_score(y_test, y_proba[:, 1])
        else:
            auc = roc_auc_score(y_test, y_proba, multi_class='ovr')
    except:
        auc = None

    resultados.append({
        'Modelo': nome,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
        'Recall': recall_score(y_test, y_pred, average='weighted'),
        'F1 Score': f1_score(y_test, y_pred, average='weighted'),
        'ROC AUC': auc
    })

    modelos_treinados[nome] = modelo
    y_preds[nome] = y_pred
    tempos.append({'Modelo': nome, 'Tempo Treinamento (s)': round(duracao, 3)})

"""ğŸ“ˆ OrganizaÃ§Ã£o e exibiÃ§Ã£o dos resultados de desempenho dos modelos

Esta cÃ©lula organiza os resultados obtidos durante a avaliaÃ§Ã£o dos modelos em um DataFrame, ordenando-os pela mÃ©trica F1 Score para destacar os modelos com melhor desempenho geral. AlÃ©m disso, registra o tempo de treinamento de cada modelo em um segundo DataFrame.
"""

#ğŸ“ˆ OrganizaÃ§Ã£o e exibiÃ§Ã£o dos resultados de desempenho dos modelos

atual_resultados = pd.DataFrame(resultados).sort_values(by='F1 Score', ascending=False)
df_resultados = atual_resultados
df_tempos = pd.DataFrame(tempos)

"""#ğŸ“‹ ComparaÃ§Ã£o visual entre modelos com base em mÃ©tricas de classificaÃ§Ã£o

Esta cÃ©lula define e executa a funÃ§Ã£o `exibir_metricas`, responsÃ¡vel por gerar visualizaÃ§Ãµes comparativas entre os modelos de machine learning avaliados. As visualizaÃ§Ãµes incluem:

- *GrÃ¡fico de barras do F1 Score*: mostra quais modelos obtiveram melhor desempenho equilibrado entre precisÃ£o e recall.
- *Mapa de calor das mÃ©tricas*: apresenta uma visÃ£o geral das principais mÃ©tricas (Accuracy, Precision, Recall, F1 Score e ROC AUC) para todos os modelos.
- *GrÃ¡fico de tempo de treinamento*: compara a eficiÃªncia temporal de cada modelo, indicando o tempo necessÃ¡rio para treinar cada um deles.

Essas visualizaÃ§Ãµes ajudam a identificar os modelos mais eficazes e eficientes  para o conjunto de dados hidrolÃ³gicos analisado.
"""

#ğŸ“‹ ComparaÃ§Ã£o visual entre modelos com base em mÃ©tricas de classificaÃ§Ã£o

def exibir_metricas(df_resultados, df_tempos):
    # Barplot - F1 Score
    plt.figure(figsize=(12, max(6, len(df_resultados) * 0.4)))
    sns.barplot(data=df_resultados, x='F1 Score', y='Modelo', hue='Modelo', palette='viridis', legend=False)
    plt.title('F1 Score por Modelo (Todos)')
    plt.tight_layout()
    plt.show()

    # Heatmap - Todas as mÃ©tricas
    plt.figure(figsize=(14, max(6, len(df_resultados) * 0.4)))
    heatmap_data = df_resultados.drop(columns='Modelo').set_index(df_resultados['Modelo']).astype(float)
    sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='.2f')
    plt.title('ğŸ“Š Heatmap Desempenho dos Modelos (Todos)')
    plt.rcParams['font.family'] = 'Segoe UI Emoji'
    plt.tight_layout()
    plt.show()

    # Tempo de treinamento
    plt.figure(figsize=(12, max(6, len(df_tempos) * 0.4)))
    sns.barplot(data=df_tempos, x='Tempo Treinamento (s)', y='Modelo', hue='Modelo', palette='magma', legend=False)
    plt.title('â±ï¸ Tempo de Treinamento por Modelo (Todos)')
    plt.rcParams['font.family'] = 'Segoe UI Emoji'
    plt.tight_layout()
    plt.show()

exibir_metricas(df_resultados, df_tempos)

"""ğŸ† VerificaÃ§Ã£o e atualizaÃ§Ã£o dos 5 melhores modelos

Esta cÃ©lula mantÃ©m um histÃ³rico dos 5 modelos com melhor desempenho com base na mÃ©trica F1 Score. Se jÃ¡ existir um arquivo melhores_modelos.csv, ele Ã© carregado e combinado com os resultados atuais. A lista combinada Ã© entÃ£o ordenada, duplicatas sÃ£o removidas e os 5 melhores modelos Ãºnicos sÃ£o selecionados. Por fim, a nova lista Ã© salva no mesmo arquivo CSV e os dados de tempo de treinamento sÃ£o cruzados para esses modelos selecionados, preparando os resultados para visualizaÃ§Ã£o futura.
"""

# ğŸ† VerificaÃ§Ã£o e atualizaÃ§Ã£o dos 5 melhores modelos

caminho_csv = 'melhores_modelos.csv'
if os.path.exists(caminho_csv):
    melhores_anteriores = pd.read_csv(caminho_csv)
    combinados = pd.concat([melhores_anteriores, atual_resultados], ignore_index=True)
    combinados = combinados.sort_values(by='F1 Score', ascending=False).drop_duplicates('Modelo').head(5)
else:
    combinados = atual_resultados.head(5)

# Salvar top 5 atualizados
combinados.to_csv(caminho_csv, index=False)

# 3. Cria a pasta para salvar os modelos (se nÃ£o existir)
os.makedirs("modelos_salvos", exist_ok=True)

# 4. Salva somente os modelos que estÃ£o no top 5
modelos_treinados = {}
y_preds = {}
tempos = []

# Cria um set com os nomes dos top 5 para facilitar a verificaÃ§Ã£o
top5_modelos = set(combinados['Modelo'])

for nome, modelo in modelos:
    if nome in top5_modelos:
        nome_arquivo = f"modelos_salvos/{nome.replace(' ', '')}.pkl"

        # Salva modelo, scaler e label encoder juntos
        joblib.dump({
            'modelo': modelo,
            'scaler': scaler,
            'label_encoder': le  # <- Aqui estÃ¡ o LabelEncoder
        }, nome_arquivo)

        print(f"Modelo, scaler e label encoder salvos em: {nome_arquivo}")

        modelos_treinados[nome] = modelo
        y_preds[nome] = y_pred  # Supondo que y_pred esteja atualizado para esse modelo
        tempos.append({'Modelo': nome, 'Tempo Treinamento (s)': round(duracao, 3)})

# Gerar top 5 resultados e tempos atualizados
top5_resultados = combinados
top5_tempos = df_tempos.merge(top5_resultados[['Modelo']], on='Modelo')
top5_tempos['Modelo'] = pd.Categorical(top5_tempos['Modelo'], categories=top5_resultados['Modelo'], ordered=True)

"""ğŸ“Š VisualizaÃ§Ã£o dos 5 melhores modelos

Esta cÃ©lula gera grÃ¡ficos para comparar visualmente o desempenho dos 5 melhores modelos selecionados. SÃ£o exibidos:

- Um grÃ¡fico de barras do F1 Score para os top 5 modelos, facilitando a comparaÃ§Ã£o direta de desempenho.
- Um heatmap com todas as mÃ©tricas de avaliaÃ§Ã£o para esses modelos, mostrando detalhes de performance de forma clara.
- Um grÃ¡fico de barras com o tempo de treinamento de cada modelo, para analisar o custo computacional associado a cada um.
"""

#ğŸ“Š VisualizaÃ§Ã£o dos 5 melhores modelos

# F1 Score dos top 5 modelos
plt.figure(figsize=(12, 6))
sns.barplot(data=top5_resultados, x='F1 Score', y='Modelo', hue='Modelo', palette='viridis', legend=False)
plt.title('F1 Score - Top 5 Modelos')
plt.tight_layout()
plt.show()

# Heatmap dos top 5 modelos
plt.figure(figsize=(12, 6))
sns.heatmap(top5_resultados.drop(columns='Modelo').set_index(top5_resultados['Modelo']).astype(float),annot=True,cmap='YlGnBu',fmt='.2f')
plt.title('ğŸ“Š Heatmap - Top 5 Modelos')
plt.tight_layout()
plt.show()

# GrÃ¡fico de tempo dos top 5 modelos
plt.figure(figsize=(12, 6))
sns.barplot(data=top5_tempos, x='Tempo Treinamento (s)', y='Modelo', hue='Modelo', palette='magma', legend=False)
plt.title('â±ï¸ Tempo de Treinamento - Top 5 Modelos')
plt.tight_layout()
plt.show()

"""ğŸ“‘ ApresentaÃ§Ã£o detalhada de mÃ©tricas preditivas para cada algoritmo

Esta cÃ©lula imprime, para cada modelo testado:

- A acurÃ¡cia mÃ©dia da validaÃ§Ã£o cruzada, que fornece uma estimativa mais robusta do desempenho geral, suavizando variaÃ§Ãµes entre divisÃµes dos dados;
- O relatÃ³rio de classificaÃ§Ã£o (classification_report), que mostra mÃ©tricas especÃ­ficas por classe (precisÃ£o, recall, f1-score), possibilitando uma avaliaÃ§Ã£o mais granular da performance;
- A matriz de confusÃ£o, que evidencia os acertos e erros de classificaÃ§Ã£o por categoria, sendo crucial para entender onde os modelos estÃ£o errando.

Essa anÃ¡lise detalhada Ã© essencial para identificar nÃ£o apenas qual modelo tem melhor desempenho geral, mas tambÃ©m quais estÃ£o mais equilibrados entre as classes e quais podem estar cometendo erros sistemÃ¡ticos.
"""

#ğŸ“‘ ApresentaÃ§Ã£o detalhada de mÃ©tricas preditivas para cada algoritmo

import numpy as np

#ğŸ“‘ ApresentaÃ§Ã£o detalhada de mÃ©tricas preditivas para cada algoritmo
for nome in top5_resultados['Modelo']:
    print(f"\nğŸ” Avaliando modelo: {nome}")

    # Tenta obter o modelo: da memÃ³ria ou do disco
    if nome in modelos_treinados:
        modelo = modelos_treinados[nome]
    else:
        try:
            caminho_modelo = f"modelos_salvos/{nome}.pkl"
            modelo = joblib.load(caminho_modelo)
            print(f"ğŸ“‚ Modelo '{nome}' carregado do disco com sucesso.")
        except FileNotFoundError:
            print(f"âŒ Modelo '{nome}' nÃ£o foi treinado nesta execuÃ§Ã£o e tambÃ©m nÃ£o foi encontrado em disco.")
            continue  # neste caso, nÃ£o tem como avaliar

    # Tenta obter as previsÃµes
    if nome in y_preds:
        y_pred = y_preds[nome]
    else:
        y_pred = modelo.predict(X_test)
        y_preds[nome] = y_pred  # salva para reutilizaÃ§Ã£o, se necessÃ¡rio

    # Se y_test ou y_pred estiverem one-hot encoded, converta para labels originais
    if y_test.ndim > 1 and y_test.shape[1] > 1:
        y_test_labels = le.inverse_transform(y_test).ravel()
    else:
        y_test_labels = y_test

    if y_pred.ndim > 1 and y_pred.shape[1] > 1:
        y_pred_labels = le.inverse_transform(y_pred).ravel()
    else:
        y_pred_labels = y_pred

    # Define as classes presentes nos dados atuais
    labels_presentes = np.unique(np.concatenate([y_test_labels, y_pred_labels]))

    # AvaliaÃ§Ã£o
    acc = accuracy_score(y_test_labels, y_pred_labels)
    prec = precision_score(y_test_labels, y_pred_labels, average='weighted', zero_division=0)
    rec = recall_score(y_test_labels, y_pred_labels, average='weighted')
    f1 = f1_score(y_test_labels, y_pred_labels, average='weighted')

    print(f"âœ… AcurÃ¡cia: {acc:.4f}")
    print(f"âœ… PrecisÃ£o: {prec:.4f}")
    print(f"âœ… RevocaÃ§Ã£o: {rec:.4f}")
    print(f"âœ… F1-Score: {f1:.4f}")

    # RelatÃ³rio de classificaÃ§Ã£o
    print("\nğŸ“„ RelatÃ³rio de ClassificaÃ§Ã£o:")
    print(classification_report(
        y_test_labels,
        y_pred_labels,
        labels=labels_presentes,
        target_names=[str(c) for c in labels_presentes],
        zero_division=0
    ))